{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation with Zygote\n",
    "- What is automatic differentiation (AD)\n",
    "- How to use chain rule to compute compilcated gradients\n",
    "- How Zygote implements this using so called adjoints\n",
    "- How to design your own adjoint\n",
    "    + How to check if it is correctly computed\n",
    "    + Some advanced examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple example\n",
    "Let's start with a simple scalar function of one variable $f(x) = 5x + 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 5x + 3;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact result of derivative with respect to `x` is of course simple constant function $\\frac{df}{dx} = 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(x) = 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Giving us the expected results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 5)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(10), df(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to compute *exact* derivatives of scalar functions (mostly) in an automatic way.\n",
    "\n",
    "In this simple case Zygote alows us to use the adjoint/transpose operator `'` to compute the same derivative but automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f(10), f'(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The important thing is that Zygote does **source to source** AD, where the derivatives itself are automatically generated functions and are treated by the compiler the same way as if you were to write them yourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "(::<b>Zygote.var\"#39#40\"</b>)(x) in Zygote at <a href=\"file:///home/honza/.julia/packages/Zygote/1GXzF/src/compiler/interface.jl\" target=\"_blank\">/home/honza/.julia/packages/Zygote/1GXzF/src/compiler/interface.jl:57</a>"
      ],
      "text/plain": [
       "(::Zygote.var\"#39#40\")(x) in Zygote at /home/honza/.julia/packages/Zygote/1GXzF/src/compiler/interface.jl:57"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@which f'(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeInfo(\n",
       "\u001b[90m1 ─\u001b[39m      goto #3\n",
       "\u001b[90m2 ─\u001b[39m      $(Expr(:meta, :inline))\n",
       "\u001b[90m3 ┄\u001b[39m      goto #4\n",
       "\u001b[90m4 ─\u001b[39m      goto #5\n",
       "\u001b[90m5 ─\u001b[39m %5 = Base.mul_int(5, 1)\u001b[36m::Int64\u001b[39m\n",
       "\u001b[90m└──\u001b[39m      goto #7\n",
       "\u001b[90m6 ─\u001b[39m      $(Expr(:meta, :inline))\n",
       "\u001b[90m7 ┄\u001b[39m      goto #8\n",
       "\u001b[90m8 ─\u001b[39m      goto #9\n",
       "\u001b[90m9 ─\u001b[39m      return %5\n",
       ") => Int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@code_typed f'(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks a little bit more complicated, but when we print the code in machine form we see that this all boils down to a function returning `5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t.text\n",
      "; ┌ @ interface.jl:57 within `#39'\n",
      "\tmovl\t$5, %eax\n",
      "\tretq\n",
      "\tnopw\t%cs:(%rax,%rax)\n",
      "; └\n"
     ]
    }
   ],
   "source": [
    "@code_native f'(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the notation of `'` operator overloading is neat, it works only for the simplest scalar functions of single variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g (generic function with 1 method)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(x,y) = 2x + 3y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "MethodError",
     "evalue": "MethodError: no method matching (::Zygote.var\"#39#40\"{typeof(g)})(::Int64, ::Int64)\nClosest candidates are:\n  #39(::Any) at /home/honza/.julia/packages/Zygote/1GXzF/src/compiler/interface.jl:57",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching (::Zygote.var\"#39#40\"{typeof(g)})(::Int64, ::Int64)\nClosest candidates are:\n  #39(::Any) at /home/honza/.julia/packages/Zygote/1GXzF/src/compiler/interface.jl:57",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[38]:1"
     ]
    }
   ],
   "source": [
    "g'(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we move to functions of more variable the thing we want to compute is `gradient`, which returns the derivative with respect to each of the input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(g, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CodeInfo(\n",
       "\u001b[90m1 ─\u001b[39m      goto #3\n",
       "\u001b[90m2 ─\u001b[39m      $(Expr(:meta, :inline))\n",
       "\u001b[90m3 ┄\u001b[39m      goto #4\n",
       "\u001b[90m4 ─\u001b[39m      goto #5\n",
       "\u001b[90m5 ─\u001b[39m %5 = Base.mul_int(3, 1)\u001b[36m::Int64\u001b[39m\n",
       "\u001b[90m│  \u001b[39m %6 = Base.mul_int(2, 1)\u001b[36m::Int64\u001b[39m\n",
       "\u001b[90m└──\u001b[39m      goto #7\n",
       "\u001b[90m6 ─\u001b[39m      $(Expr(:meta, :inline))\n",
       "\u001b[90m7 ┄\u001b[39m %9 = Core.tuple(%6, %5)\u001b[36m::Tuple{Int64,Int64}\u001b[39m\n",
       "\u001b[90m└──\u001b[39m      goto #8\n",
       "\u001b[90m8 ─\u001b[39m      return %9\n",
       ") => Tuple{Int64,Int64}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@code_typed gradient(g, 1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which also works on the previous example with function `f`, but instead of returning single number it gives a tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(f, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the backpropagation for a single layer neural network\n",
    "$$\n",
    "n(x) = Wx + b\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = rand(2, 3), rand(2);\n",
    "net(x) = W*x .+ b;\n",
    "agg(n) = sum(n); # simple aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 1.024699723026722\n",
       " 3.078770609798341"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net([1,3,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the gradient is computed with respect to the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.6579457323925992, 1.4986819280476997, 0.898998435768311],)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient((x) -> agg(net(x)), [1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you write it down this should correspont to taking the sum of rows of the weight matrix `W`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×3 Array{Float64,2}:\n",
       " 1.65795  1.49868  0.898998"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(W,dims=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the field of machine learning we are however usually interested in taking derivatives with respect to the weights and biases. For that Zygote uses the concept of `Params` parameters, which are in turn used in the Flux library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[0.8242805451868016 0.6935345698171789 0.3575917782149711; 0.8336651872057976 0.8051473582305209 0.5414066575533398], [0.12011868865849706, 0.28705432802353603]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = Params([W, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the gradient instead, returns a dictionary with entries for each paramter in `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grads(...)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads = gradient(() -> agg(net([1,2,3])), p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient dictionary can be indexed directly with the parameter we want the gradient to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Array{Float64,2}:\n",
       " 1.0  2.0  3.0\n",
       " 1.0  2.0  3.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grads[W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the gradient is a matrix, even though one would expect it to be vector as you were though in calculus. This representation of gradient is more convenient as the update step of gradient descent algortihm looks really simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×3 Array{Float64,2}:\n",
       " 0.814281  0.673535  0.327592\n",
       " 0.823665  0.785147  0.511407"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "α = 0.01\n",
    "W .-= α.*grads[W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But keep in mind that from calculus point of view we are still working with scalar functions of multiple variables \n",
    "$$\n",
    "W = \n",
    "\\begin{pmatrix}\n",
    "W_{11} & W_{12} & W_{13}\\\\\n",
    "W_{21} & W_{22} & W_{23}\n",
    "\\end{pmatrix} \\\\\n",
    "b = \n",
    "\\begin{pmatrix}\n",
    "b_{1} \\\\\n",
    "b_{2}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "and gradients should be in a vector form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6-element Array{Float64,1}:\n",
       " 1.0\n",
       " 1.0\n",
       " 2.0\n",
       " 2.0\n",
       " 3.0\n",
       " 3.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec(grads[W])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element FillArrays.Fill{Float64,1,Tuple{Base.OneTo{Int64}}}:\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec(grads[b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Under the hood (forward/backward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look now at how this automatic differentiation works internally. Let's define more complicated nested function such as\n",
    "$$\n",
    "s(x) = 5sin(x^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s (generic function with 1 method)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(x) = sin(x^2) * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to point out that there is really no much difference from how Zygote computes the derivatives and how you may go about it. What we do is applying the following basic rules to compute the result.\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{d}{dx} x &= 1 \\\\\n",
    "\\frac{d}{dx} (-u) &= - \\frac{du}{dx} \\\\\n",
    "\\frac{d}{dx} (u + v) &= \\frac{du}{dx} + \\frac{dv}{dx} \\\\\n",
    "\\frac{d}{dx} (uv) &= v \\frac{du}{dx} + u \\frac{dv}{dx} \\\\\n",
    "\\frac{d}{dx} (u / v) &= (v \\frac{du}{dx} - u \\frac{dv}{dx}) / v^2 \\\\\n",
    "\\frac{d}{dx} u^n &= n u^{n-1} \\\\\n",
    "\\frac{d}{dx} u(v) &= \\frac{du}{dv} \\frac{dv}{dx} \\\\\n",
    "\\vdots\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing to understand is that due to Julia's superior meta language capabilities, it is able to understand the code of a function `s` and decompose it in order to apply these rules by pattern matching. Result of which is so called Wengert list of `s`\n",
    "```julia\n",
    "y1 = x ^ 2\n",
    "y2 = sin(y1)\n",
    "y3 = y2 * 5\n",
    "```\n",
    "In other words, we have been able to produce a list of assigments, each of which we are able to differentiate using the standard calculus toolbox. In order to compute the derivative itself we just have to chain the elementary derivatives $\\frac{dy_1}{dx}, \\frac{dy_2}{dy_1}, \\frac{ds}{dy_2}$ and voila \n",
    "$$\n",
    "\\frac{ds}{dx} = \\frac{dy_1}{dx} \\times \\frac{dy_2}{dy_1} \\times \\frac{ds}{dy_2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the fact that multiplication is asociative, there are two ways how to compute the derivate. \n",
    "We can go either from either evaluating first $\\frac{dy_1}{dx} \\times \\frac{dy_2}{dy_1}$ first, or $\\frac{dy_2}{dy_1} \\times \\frac{ds}{dy_2}$?\n",
    "\n",
    "It's easier to see the distinction if we think algorithmically. Given some\n",
    "enormous Wengert list with $n$ instructions, we have two ways to differentiate\n",
    "it:\n",
    "\n",
    "### Forward\n",
    "Start with the known quantity $\\frac{dy_0}{dx} = \\frac{dx}{dx} = 1$\n",
    "at the beginning of the list. Look up the derivative for the next instruction\n",
    "$\\frac{dy_{i+1}}{dy_i}$ and multiply out the top, getting $\\frac{dy_1}{dx}$,\n",
    "$\\frac{dy_2}{dx}$, ... $\\frac{dy_{n-1}}{dx}$, $\\frac{dy}{dx}$. Because we\n",
    "walked forward over the Wengert list, this is called *forward mode*. Each\n",
    "intermediate derivative $\\frac{dy_i}{dx}$ is known as a *perturbation*.\n",
    "\n",
    "### Reverse\n",
    "Start with the known quantity $\\frac{dy}{dy_n} = \\frac{dy}{dy} = 1$\n",
    "at the end of the list. Look up the derivative for the previous instruction\n",
    "$\\frac{dy_i}{dy_{i-1}}$ and multiply out the bottom, getting\n",
    "$\\frac{dy}{dy_n}$, $\\frac{dy}{dy_{n-1}}$, ... $\\frac{dy}{dy_1}$,\n",
    "$\\frac{dy}{dx}$. Because we walked in reverse over the Wengert list, this is\n",
    "called *reverse mode*. Each intermediate derivative $\\frac{dy}{dy_i}$ is known\n",
    "as a *sensitivity*.\n",
    "\n",
    "\n",
    "Zygote uses the latter method as there are huge performance benefits, when computing derivatives with respect to milions of parameters as is the case with ML models.\n",
    "- taking derivatives of scalar function with respect to more than one variable requires only one pass\n",
    "- $\\frac{dy_i}{dx}$ is a huge Jacobi matrix (`length(y_i) * length(x)`), whereas $\\frac{dy}{dy_i}$ is represented more compactly (`length(y) * length(y_i)`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How is it implemented?\n",
    "Even though the way the Wengert lists are created is really important here we will focus only on the differentiation itself.\n",
    "\n",
    "Going back to our function `s` and suppose that Zygote has already parsed the function into a Wengert list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "s (generic function with 1 method)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1(x) = x ^ 2\n",
    "w2(x) = sin(x)\n",
    "w3(x) = x * 5.0\n",
    "\n",
    "function s(x)\n",
    "    y1 = w1(x)\n",
    "    y2 = w2(y1)\n",
    "    y3 = w3(y2)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as a reference, here is the result and gradient that we want to compute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.4395165833253265, -38.30637921293538)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s(4.0), gradient(s, 4.0)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Zygote `gradient` is just a wrapper for internal function called `pullback` (or more precisely even more internal `_pullback` and `Pullback` type), which returns both the value of a given function and something called **adjoint** or **adjoint function** representing the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.4395165833253265, Zygote.var\"#37#38\"{typeof(∂(s))}(∂(s)))"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_val, s_back = pullback(s, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should we pass that adjoint function to get our gradient? For that we need to understand what the adjoint represents.\n",
    "\n",
    "Looking back to the differentiated Wengert list\n",
    "$$\n",
    "\\frac{ds}{dx} = \\frac{dy_1}{dx} \\times \\frac{dy_2}{dy_1} \\times \\frac{dy_3}{dy_2},\n",
    "$$\n",
    "we can pull back the adjoint for every other intermediate result in the list, while feeding the interemediate value to the next function in the list as the derivative for `y2` is computed not at `x = 4.0` but at `y1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.4395165833253265, Zygote.var\"#37#38\"{typeof(∂(w3))}(∂(w3)))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_val, y1_back = pullback(w1, 4.0)\n",
    "y2_val, y2_back = pullback(w2, y1_val)\n",
    "y3_val, y3_back = pullback(w3, y2_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected `y3_val == s_val`. In this way we have thus evaluated the function `s` itself, however we got also the adjoint functions for each intermediate result of that Wengert list, which can be composed in **reverse** order to compute the derivative of the whole list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-38.30637921293538"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_back(y2_back(y3_back(1.0)[1])[1])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the derivative we were looking for, where the input `1.0` corresponds to the fact that $\\frac{dy_3}{dy_3} = 1$ and what the adjoint function is really computing is the multiplication of two subsequent derivatives/gradients in the chain, i.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.0,)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y3_back(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computes $\\frac{dy_3}{dy_2} \\times \\frac{dy_3}{dy_3} = \\frac{ds}{dy_2}$, this result is plug again into the pullback of `y2` to compute $\\frac{dy_2}{dy_1} \\times \\frac{dy_3}{dy_2} = \\frac{ds}{dy_1}$ and so on until the derivative is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.788297401616923,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y2_back(y3_back(1.0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-38.30637921293538"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_back(y2_back(y3_back(1.0)[1])[1])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjoint functions are thus not derivatives/gradients themselves, but they implemented as the multiplication of two derivatives/gradients in the chain. In other words, given a derivative with respect to it's output, they return derivative with respect to it's inputs.\n",
    "\n",
    "In case of scalar functions of one variable, this is all there is to know about how the adjoint works, however things get a little messy, when we start working with vector functions of multiple variables. such as the following example of composition of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "f(x) &= \n",
    "\\begin{pmatrix}\n",
    "{x_{1} + x_{2}} \\\\\n",
    "{x_{1}x_{2}}\n",
    "\\end{pmatrix} \\\\\n",
    "g(x) &= \n",
    "\\begin{pmatrix}\n",
    "e^{(x_{1} + x_{2})} \\\\\n",
    "{\\ln(x_{1}) + \\ln(x_{2})}\n",
    "\\end{pmatrix} \\\\\n",
    "h(x) &= g\\left(f(x)\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "For comparison I have precomputed the Jacobi matrices for each of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = [x[1] + x[2], x[1]*x[2]]\n",
    "df(x) = [1.0 1.0; x[2] x[1]];\n",
    "\n",
    "g(y) = [exp(y[1] + y[2]), log(y[1])+log(y[2])]\n",
    "dg(y) = [exp(y[1] + y[2]) exp(y[1] + y[2]); 1/y[1] 1/y[2]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h (generic function with 1 method)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(x) = g(f(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the derivative of $h$ we use the chain rule\n",
    "$$\n",
    "h'(x) = g'\\left(f(x)\\right)f'(x),\n",
    "$$\n",
    "where $g'$ and $f'$ are the corresponding Jacobi matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dh (generic function with 1 method)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh(x) = dg(f(x))*df(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we expect to compute in this case when we ask Zygote to compute pullback of vector functions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3.0, 2.0], Zygote.var\"#37#38\"{typeof(∂(f))}(∂(f)))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val = [1.0, 2.0]\n",
    "f_val, f_back = pullback(f, x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Array{Float64,2}:\n",
       " 1.0  1.0\n",
       " 2.0  1.0"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we are working with vector functions of multiple variables, we the input for adjoint corresponds to either if we are computing the derivative of $f_1$ \n",
    "$$\n",
    "\\nabla_x f_1 =  \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_1}{\\partial x_1} &\n",
    "\\frac{\\partial f_1}{\\partial x_2}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.0, 1.0],)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_back([1.0, 0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or $f_2$\n",
    "$$\n",
    "\\nabla_x f_2 =  \n",
    "\\begin{pmatrix}\n",
    "\\frac{\\partial f_2}{\\partial x_1} &\n",
    "\\frac{\\partial f_2}{\\partial x_2}\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2.0, 1.0],)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_back([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjoint function now corresponds to left multiplication the Jacobi matrix with the gradient to it's output.\n",
    "$$\n",
    "\\nabla \\cdot f'\n",
    "$$\n",
    "( in the code of adjoints there is usually additional transposition of ∇ to a row vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain the whole Jacobi matrix of a vector function we have to evaluate the reverse step multiple times, however this is usually not necessary in ML applications. Now back to the function `h`, whose Wenger list representation would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function h(x)\n",
    "\ty1 = f(x)\n",
    "\ty2 = g(y1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([148.4131591025766, 1.791759469228055], Zygote.var\"#37#38\"{typeof(∂(g))}(∂(g)))"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_val, y1_back = pullback(f, x_val)\n",
    "y2_val, y2_back = pullback(g, y1_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should correspond to running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([148.4131591025766, 1.791759469228055], Zygote.var\"#37#38\"{typeof(∂(h))}(∂(h)))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_val, h_back = pullback(h, x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing to compute $\\nabla_x f_2$, we get three time the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.3333333333333333, 0.8333333333333333],)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_back([0.0, 1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.3333333333333333, 0.8333333333333333],)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_back(y2_back([0.0, 1.0])[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 1.3333333333333333\n",
       " 0.8333333333333333"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dh(x_val)[2,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Footnote: Matrix valued functions\n",
    "Now that we know how Zygote works on vector functions, let's complicate the things a little bit for ourselves with matrix-matrix valued functions such as the linear layer of neural network from the beggining. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n (generic function with 1 method)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W, b = rand(2, 3), rand(2);\n",
    "n(X) = W*X .+ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X` is now a matrix, representing a batch of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×10 Array{Float64,2}:\n",
       " 0.329415    0.461212  0.13606   0.0893104  …  0.807367  0.203848  0.531409\n",
       " 0.00551086  0.565459  0.957635  0.362832      0.656018  0.822869  0.297294\n",
       " 0.919522    0.617153  0.829805  0.740371      0.808664  0.76231   0.90353"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val = rand(3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though that is usually not the case, let's try to compute the derivative of $n$ with respect to the matrix of values $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.8745972125649726 1.7936400679733917 … 1.7180494740487249 2.064463624172422; 0.8881247845229077 0.8233833186720201 … 0.7403741536734852 1.0110236041265654], Zygote.var\"#37#38\"{typeof(∂(n))}(∂(n)))"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val, n_back = pullback(n, X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the output another matrix of size 2x10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×10 Array{Float64,2}:\n",
       " 1.8746    1.79364   1.72908   1.55523  …  2.25938  1.71805   2.06446\n",
       " 0.888125  0.823383  0.734574  0.65036     1.14072  0.740374  1.01102"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine function `n` to be a part of big model with some scalar loss function, whose derivative we compute in reverse mode. When we get to it we have already collected some gradient with respect to the output of `n`, which has the same size as `n_val` (rememmber the `vec` transformation of matrices to vector is just a matter of conveniece), therefore the input for our adjoint function `n_back` will have the same dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×10 Array{Float64,2}:\n",
       " 0.611034  0.271175  0.283491  0.947586  …  0.906588  0.271459  0.948848\n",
       " 0.314994  0.891307  0.96543   0.757883     0.235232  0.328397  0.490567"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Δ = rand(size(n_val)...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Giving us the same dimensions as the input of the matrix function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3×10 Array{Float64,2}:\n",
       " 0.723871   0.793104   0.850247   1.29172  …  0.928261   0.439914   1.12496\n",
       " 0.0657984  0.0403055  0.0426331  0.10602     0.0941948  0.0320163  0.102196\n",
       " 0.668416   0.683785   0.732171   1.17535     0.872146   0.394035   1.03869"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_back(Δ)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In calculus however we are usually not taught how to differentiate such functions, as they are just hidden vector functions of multiple variables, whose Jacobi matrices are computed in the same way, however making sense of it is may not be as straightforward for everyone. \n",
    "\n",
    "Take for example the matrix-matrix multiplication in the function `n`. Suppose there is some projection from matrix to vector (in julia this is the `vec` function which gives the matrix in column major order) giving us vectors $n_v$, $x_v$.\n",
    "$$\n",
    "n_v(x_v) = n_v\n",
    "\\begin{pmatrix}\n",
    "x_{1,1} \\\\\n",
    "\\vdots  \\\\\n",
    "x_{3,10}\n",
    "\\end{pmatrix}\n",
    "= \n",
    "\\begin{pmatrix}\n",
    "n_{1,1} \\\\\n",
    "\\vdots  \\\\\n",
    "n_{2,10}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "Now we can compute the jacobian for such \"vectorized\" function\n",
    "$$\n",
    "n_v'(x_v) = W \\otimes I_{3},\n",
    "$$\n",
    "where $I_{3}$ is identity 3x3 and $\\otimes$ is Kronecker/tensor product of matrices. This may look complicated but in reality, when we multiply this from the left with the \"vectorized\" gradient $\\Delta_v$, we get that the adjoint operation boils down to a simple matrix multiplication of\n",
    "$$\n",
    "\\Delta^TW\n",
    "$$\n",
    "\n",
    "It is often the case, that the Jacobi matrix of \"vectorized\" function does have a nice structure, that allows us to write the adjoints in a more effective way instead of computing the whole Jacobi matrix and doing the multiplication. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to write your own custom adjoints\n",
    "Though the automatic differentiation in Zygote is really powerfull tool, sometimes you will be forced to write your own adjoint functions. The reasons for this may be \n",
    "- performance (functions writte in a way that Zygote understands it may not be most performant solution)\n",
    "- unsupported operations (array mutation)\n",
    "- replacing some operations when computing reverse (argmax <-> softmax)\n",
    "- bragging rights\n",
    "\n",
    "The users are encouraged to write their own adjoints in order to get the most out of this great tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote: @adjoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple examples\n",
    "Let's start again with some simple scalar function of two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "add (generic function with 2 methods)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(a, b) = a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjoints are defined using the `@adjoint` macro, which is a shorthand for defining `pullback(typeof(:add), a, b)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "@adjoint add(a, b) = add(a, b), Δ -> (Δ, Δ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first element of the tuple allows us to redefine function `add`, though here we just call the function itself. The second element is the adjoint function itself, which in this case is really simple, returning the gradient/derivative multiplied by one for each of the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll be back!?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_val, add_back = pullback(add, 2, 3)\n",
    "add_back(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In situation where it may be useful, we can redefine the function itself just for the purposes of AD. This can be usefull in cases, where the code of the adjoint and the function itself is similar and we would like to reuse some of the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "@adjoint add(a, b) = begin\n",
    "    println(\"I'll be back!?\")\n",
    "    add(a, b) \n",
    "end, Δ -> (Δ, Δ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the original function defintion is kept untouched "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradcheck\n",
    "How do we check if the adjoint we have defined is correct?\n",
    "- Zygote itself (only if the automatic pullback works)\n",
    "- numerical derivatives\n",
    "- by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats (global variables)\n",
    "If there are some global variables used in the function or if we use a type as a functor (calling the type itself), which is often the case with neural network modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: incomplete: \"function\" at none:1 requires end",
     "output_type": "error",
     "traceback": [
      "syntax: incomplete: \"function\" at none:1 requires end",
      ""
     ]
    }
   ],
   "source": [
    "function addg(a) \n",
    "    let b = 3;\n",
    "    a + b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, ∂(addg))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addg_val, addg_back = Zygote._pullback(addg, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(nothing, 2)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addg_back(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@adjoint add(a, b) = add(a, b), Δ -> (Δ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best practices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
