{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flux.jl\n",
    "\n",
    "In this notebook, we will go through the basic usage of the Flux library that offers basic tools for building neural networks in Julia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flux offers gradient and easy computation of the chain rule. It already has `gradient` for most standard functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x) = 3x^2 + 2x + 1;\n",
    "df(x) = 6x + 2;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " -4\n",
       "  8\n",
       " 20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.([-1,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(f,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " -4\n",
       "  8\n",
       " 20"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df(x) = gradient(f,x)[1]\n",
    "df.([-1,1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second order derivative is easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 6\n",
       " 6\n",
       " 6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2f(x) = gradient(df, x)[1]\n",
    "d2f.([-1,1,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient of composite functions works naturaly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "g (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g(x) = sin(f(x)) + cos(f(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.6023373578795135,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(g, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also differentiate multivariate functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "h (generic function with 1 method)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h(x, y) = 2 .*x .*y .+ x.^2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(h, 1, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the output has to be scalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 5\n",
       " 0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b = [1, 0], [2, 1]\n",
    "h(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ErrorException",
     "evalue": "Output should be scalar; gradients are not defined for output [5, 0]",
     "output_type": "error",
     "traceback": [
      "Output should be scalar; gradients are not defined for output [5, 0]",
      "",
      "Stacktrace:",
      " [1] error(::String) at ./error.jl:33",
      " [2] sensitivity(::Array{Int64,1}) at /home/vit/.julia/packages/Zygote/1GXzF/src/compiler/interface.jl:50",
      " [3] gradient(::Function, ::Array{Int64,1}, ::Vararg{Array{Int64,1},N} where N) at /home/vit/.julia/packages/Zygote/1GXzF/src/compiler/interface.jl:54",
      " [4] top-level scope at In[34]:1"
     ]
    }
   ],
   "source": [
    "gradient(h, a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is the `params` function, that collects the trainable parameters of its arguments. By default, every array structure is trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[1, 3, 5]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params([1,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0; 0.0 0.0 … 0.0 0.0]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(zeros(3,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "f (generic function with 2 methods)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [2, 1];\n",
    "y = [2, 0];\n",
    "f(x,y) = sum((x .- y).^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[2, 1]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[2, 1], [2, 0]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary use of this is that it provides an iterable over all trainable parameters of the arguments (e.g. a NN layer). Then, we can collect gradient with respect to all the trainable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grads(...)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is like map()\n",
    "gs = gradient(params(x, y)) do\n",
    "    f(x,y)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which is simillar to this, only this already gives the values of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Tuple{Int64,Int64},1}:\n",
       " (4, -4)\n",
       " (4, -4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map(x->gradient(f,x[1],y[2]), params(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, `gs` containts gradients of `f` with respect to `x` and `y` evaluated at `x` and `y` values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 0\n",
       " 2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       "  0\n",
       " -2"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs[y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assigments are given the whole variable, not only its value. Honza will talk about this more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "KeyError: key [2, 1] not found",
     "output_type": "error",
     "traceback": [
      "KeyError: key [2, 1] not found",
      "",
      "Stacktrace:",
      " [1] getindex at ./abstractdict.jl:603 [inlined]",
      " [2] getindex(::Zygote.Grads, ::Array{Int64,1}) at /home/vit/.julia/packages/Zygote/1GXzF/src/compiler/interface.jl:142",
      " [3] top-level scope at In[62]:2"
     ]
    }
   ],
   "source": [
    "z = [2,1]\n",
    "gs[z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Int64,1}:\n",
       " 0\n",
       " 2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x\n",
    "gs[z]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×5 Array{Float64,2}:\n",
       " 0.423072  0.61746   0.741955   0.564364  0.677725\n",
       " 0.952557  0.483217  0.0400681  0.517426  0.00170474"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W = rand(2, 5) # weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.9660418636262085\n",
       " 0.31926599874752615"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = rand(2) # bias vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.3844902613055385"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setup the prediction and loss functions\n",
    "predict(x) = W*x .+ b\n",
    "\n",
    "function loss(x, y)\n",
    "  ŷ = predict(x)\n",
    "  sum((y .- ŷ).^2)\n",
    "end\n",
    "\n",
    "x, y = rand(5), rand(2) # Dummy data\n",
    "loss(x, y) # ~ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us gradients of `loss`, with respect to `W` and `b` and evaluated at `x` and `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grads(...)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = gradient(() -> loss(x, y), params(W, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 4.158329854178062\n",
       " 0.4962397294389196"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs[b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×5 Array{Float64,2}:\n",
       " 0.971839  2.86115   3.59914   1.85862   2.56374\n",
       " 0.115976  0.341439  0.429508  0.221801  0.305947"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs[W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do one step of gradient descent on `W`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×5 Array{Float64,2}:\n",
       " 0.0343368  -0.526999  -0.697701  -0.179084  -0.347771\n",
       " 0.906167    0.346641  -0.131735   0.428705  -0.120674"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ΔW = gs[W]\n",
    "W .-= 0.1 .* ΔW # equivalent to W .= W .- 0.1 .* ΔW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0340910128981746"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building your own layer\n",
    "\n",
    "This is a general way to constructing your own layers. This `Affine` layer is equivalent to the generic `Dense` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Affine\n",
    "  W\n",
    "  b\n",
    "end\n",
    "\n",
    "# constructor\n",
    "Affine(in::Integer, out::Integer) =\n",
    "  Affine(randn(out, in), randn(out))\n",
    "\n",
    "# Overload call, so the object can be used as a function\n",
    "(m::Affine)(x) = m.W * x .+ m.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element Array{Float64,1}:\n",
       "  2.3171259850207164\n",
       "  0.29932674159907513\n",
       "  3.414888241112594\n",
       "  1.8948799753517813\n",
       " -1.5603099414126003"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Affine(10, 5)\n",
    "x = rand(10)\n",
    "a(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how you would train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 2 methods)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = randn(5)\n",
    "loss(x) = Flux.mse(a(x), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "mse(ŷ, y)\n",
       "\\end{verbatim}\n",
       "Return the mean squared error between ŷ and y; calculated as \\texttt{sum((ŷ .- y).\\^{}2) / length(y)}.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> Flux.mse([0, 2], [1, 1])\n",
       "1//1\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "mse(ŷ, y)\n",
       "```\n",
       "\n",
       "Return the mean squared error between ŷ and y; calculated as `sum((ŷ .- y).^2) / length(y)`.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> Flux.mse([0, 2], [1, 1])\n",
       "1//1\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  mse(ŷ, y)\u001b[39m\n",
       "\n",
       "  Return the mean squared error between ŷ and y; calculated as \u001b[36msum((ŷ .-\n",
       "  y).^2) / length(y)\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> Flux.mse([0, 2], [1, 1])\u001b[39m\n",
       "\u001b[36m  1//1\u001b[39m"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Flux.mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.2645227146100988, -1.1568070613619492, -0.7253953299846515, 2.428544523863895, 2.3968036081983337, -0.7109940253003317, -2.8376733229419826, 2.240541381593441, 6.365583385100586, -0.28770714590399205],)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient(loss, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grads(...)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = gradient(() -> loss(x), params(a.W, a.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5×10 Array{Float64,2}:\n",
       "  0.731564     0.79207      0.602092    …   0.783689     0.217362\n",
       " -0.00500343  -0.00541725  -0.00411792     -0.00535993  -0.00148662\n",
       "  1.23973      1.34227      1.02033         1.32807      0.368349\n",
       "  0.712987     0.771957     0.586803        0.763788     0.211842\n",
       " -0.393937    -0.426518    -0.324218       -0.422005    -0.117046"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs[a.W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this is a bit awkward, instead, we would like to be able to call `params(a)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that, we need to call the `@functor` macro on the structure we have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@functor Affine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does a lot of stuff in the background, enables collection of parameters and also GPU operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[0.9059637991871812 -1.204948729220692 … 1.6104494996658223 -1.5921129291004092; 0.4464281307087328 -0.30004611126937125 … -0.9451035515937852 0.2793767943057875; … ; -0.06837077662362061 0.4395277607620131 … 2.4157883658205264 -1.2245245948404366; -1.3992451498455167 0.9638225402250484 … -1.5880690601500653 -1.108842454225542], [0.5973366619951, 0.4343777167145093, 0.20764423746494565, 0.5725076729533816, -0.3634501916507353]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grads(...)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs = gradient(() -> loss(x), params(a))\n",
    "gs[a.W]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define which parameters are trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.trainable(a::Affine) = (a.W,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Params([[0.9059637991871812 -1.204948729220692 … 1.6104494996658223 -1.5921129291004092; 0.4464281307087328 -0.30004611126937125 … -0.9451035515937852 0.2793767943057875; … ; -0.06837077662362061 0.4395277607620131 … 2.4157883658205264 -1.2245245948404366; -1.3992451498455167 0.9638225402250484 … -1.5880690601500653 -1.108842454225542]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be also achieved with `@functor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "Flux.@functor Affine (W,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer chaining\n",
    "\n",
    "Is useful for constructing larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(5, 10, relu), Dense(10, 1, σ))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(\n",
    "    Dense(5,10,relu),\n",
    "    Dense(10,1, sigmoid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5-element view(::Array{Float64,2}, 1, [3, 4, 5, 6, 9]) with eltype Float64:\n",
       " -3.0\n",
       " -3.0\n",
       " -3.0\n",
       " -3.0\n",
       " -3.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = randn(5,10) # a batch of 10 5 dimensional samples\n",
    "y = rand([0,1], 1, 10) # labels\n",
    "x[1,vec(y).==1] .= -3 # make a connection between the labels and the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 Array{Float32,2}:\n",
       " 0.392138  0.438458  0.133978  0.167633  …  0.441877  0.129883  0.581017"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimisers\n",
    "\n",
    "Serve to steer the gradient optimization in the correct direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is mean binary crossentropy, ideal for classification. Logit for better stability.\n",
    "\n",
    "\\begin{equation*}\n",
    "    l(y,\\hat{y}) = - \\frac{1}{N} \\sum_{i=1}^N y_i \\log(\\hat{y}_i) +  (1-y_i) \\log(1-\\hat{y}_i)\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x,y) = sum(Flux.logitbinarycrossentropy.(m(x), y))/size(y,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7894722f0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we collect the gradients with respect to parameters of `m`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Grads(...)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θ = params(m)\n",
    "grads = gradient(() -> loss(x,y), θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux.Optimise: update!\n",
    "\n",
    "η = 0.1 # Learning Rate\n",
    "for p in θ\n",
    "  update!(p, -η .* grads[p])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7920066f0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADAM optimiser. Remember not to restart it everytime you optimize with it as it has memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADAM(0.01, (0.9, 0.999), IdDict{Any,Any}())"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = ADAM(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IdDict{Any,Any} with 0 entries"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in θ\n",
    "  update!(opt, p, grads[p])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78266543f0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IdDict{Any,Any} with 4 entries:\n",
       "  Float32[-0.252127 0.2386… => (Float32[0.00307778 0.0042404 … -0.00307911 0.00…\n",
       "  Float32[-0.00531625]      => (Float32[0.00468375], Float32[2.19375f-6], (0.81…\n",
       "  Float32[0.00891607, -0.0… => (Float32[-0.00108392, 0.0010806, 0.00114829, 0.0…\n",
       "  Float32[0.475649 0.23809… => (Float32[-0.000741009 -0.000553543 … -0.00084561…"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a lot more to optimisers, you can choose from a wide range, also different decay strategies, which can be composed with optimisers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "There are functions that enable easy training via batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not do this, you can use train!\n",
    "for i in 1:100\n",
    "    for p in θ\n",
    "      update!(opt, p, grads[p])\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate some more data and split it into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Flux.Data.DataLoader(([-0.20319554973784926 0.7664197124768922 … -3.0 -3.0; 0.18976232034546922 0.9908676053541495 … 0.17432628446833415 0.4567179964371301; … ; 0.9652189483130672 0.48193225077941515 … -1.7587692155972963 -1.0977898713415803; 0.583656907469228 0.2669864497924328 … 0.8833551098769331 0.750108352225107], [0 0 … 1 1]), 128, 1000, true, 1000, [1, 2, 3, 4, 5, 6, 7, 8, 9, 10  …  991, 992, 993, 994, 995, 996, 997, 998, 999, 1000], false)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = randn(5,1000)\n",
    "Y = rand([0,1], 1, 1000) # labels\n",
    "X[1,vec(Y).==1] .= -3 # make a connection between the labels and the data\n",
    "data = Flux.Data.DataLoader(X, Y, batchsize=128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define a callback function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "#14 (generic function with 1 method)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cb = () -> println(loss(x,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the model and optimiser from the above code. This will compute gradients and update the parameters on each batch in data twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 1\n",
      "└ @ Main /home/vit/.julia/packages/Flux/Fj3bt/src/optimise/train.jl:121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.77472967\n",
      "0.7671571\n",
      "0.7598773\n",
      "0.752929\n",
      "0.7461692\n",
      "0.7392982\n",
      "0.73244995\n",
      "0.72546804\n",
      "0.7185718\n",
      "0.7115805\n",
      "0.7046176\n",
      "0.6978354\n",
      "0.6910521\n",
      "0.6840501\n",
      "0.6770234\n",
      "0.6698472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Epoch 2\n",
      "└ @ Main /home/vit/.julia/packages/Flux/Fj3bt/src/optimise/train.jl:121\n"
     ]
    }
   ],
   "source": [
    "θ = params(m)\n",
    "Flux.@epochs 2 Flux.train!(loss, θ, data, opt; cb = cb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that `train!` calls `loss(d...)` on each element of `data`. Therefore if you only pass one argument to the loss, function, you have to pass `(x,)` not just `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 Array{Float32,2}:\n",
       " 0.0999795  0.257127  0.436714  0.432262  …  0.20172  0.438776  0.446962"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 Array{Int64,2}:\n",
       " 0  0  1  1  1  1  0  0  1  0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter freezing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also freeze some parameters, i.e. exclude them from `params` and therefore from optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length(θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θ = params(m[1])\n",
    "length(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can delete a specific one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "θ = params(m)\n",
    "delete!(θ, m[2].b)\n",
    "length(θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSON.@save \"model.bson\" m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSON.@load \"model.bson\" m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also save/load the individual weights - see documentation. I recommend to save the optimizer as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(5, 10, relu), Dense(10, 1, σ))"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Chain(\n",
    "    Dense(5,10,relu),\n",
    "    Dense(10,1, sigmoid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×5 Array{Float32,2}:\n",
       "  0.278172   -0.393357   -0.0831804  -0.127154  -0.338539\n",
       " -0.516893   -0.155446   -0.170235   -0.395183  -0.37047\n",
       "  0.392659   -0.302037    0.412356    0.122162   0.573642\n",
       " -0.612919   -0.591922   -0.590492   -0.275072  -0.318672\n",
       "  0.0477564   0.0205548  -0.27069     0.393052   0.570552\n",
       "  0.599294   -0.165937    0.563544   -0.291239   0.197528\n",
       " -0.443511    0.588622    0.127816    0.449827  -0.0208685\n",
       " -0.175823    0.24797     0.470389    0.566752   0.226565\n",
       " -0.435666   -0.170884   -0.597009    0.174136   0.165709\n",
       " -0.549613    0.0434508   0.0475399   0.129501   0.163205"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[1].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(5, 10, relu), Dense(10, 1, σ))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using CuArrays\n",
    "m = m |> gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×5 CuArray{Float32,2,Nothing}:\n",
       "  0.278172   -0.393357   -0.0831804  -0.127154  -0.338539\n",
       " -0.516893   -0.155446   -0.170235   -0.395183  -0.37047\n",
       "  0.392659   -0.302037    0.412356    0.122162   0.573642\n",
       " -0.612919   -0.591922   -0.590492   -0.275072  -0.318672\n",
       "  0.0477564   0.0205548  -0.27069     0.393052   0.570552\n",
       "  0.599294   -0.165937    0.563544   -0.291239   0.197528\n",
       " -0.443511    0.588622    0.127816    0.449827  -0.0208685\n",
       " -0.175823    0.24797     0.470389    0.566752   0.226565\n",
       " -0.435666   -0.170884   -0.597009    0.174136   0.165709\n",
       " -0.549613    0.0434508   0.0475399   0.129501   0.163205"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m[1].W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×10 CuArray{Float32,2,Nothing}:\n",
       " 0.585089  0.705926  0.423339  0.595285  …  0.583601  0.57859  0.448476"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = randn(5,10) |> gpu\n",
    "m(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before saving your model, move it to CPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Chain(Dense(5, 10, relu), Dense(10, 1, σ))"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = m |> cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv((3, 3), 1=>4, relu)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = Conv((3,3), 1=>4, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m! \u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mert \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mDims \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22mTranspose ∇\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m_data ∇\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m_data!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Conv(size, in => out, σ = identity; init = glorot_uniform,\n",
       "     stride = 1, pad = 0, dilation = 1)\n",
       "\\end{verbatim}\n",
       "Standard convolutional layer. \\texttt{size} should be a tuple like \\texttt{(2, 2)}. \\texttt{in} and \\texttt{out} specify the number of input and output channels respectively.\n",
       "\n",
       "Data should be stored in WHCN order (width, height, \\# channels, batch size). In other words, a 100×100 RGB image would be a \\texttt{100×100×3×1} array, and a batch of 50 would be a \\texttt{100×100×3×50} array.\n",
       "\n",
       "\\section{Examples}\n",
       "Apply a \\texttt{Conv} layer to a 1-channel input using a 2×2 window size, giving us a 16-channel output. Output is activated with ReLU.\n",
       "\n",
       "\\begin{verbatim}\n",
       "size = (2,2)\n",
       "in = 1\n",
       "out = 16\n",
       "Conv(size, in => out, relu)\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "Conv(size, in => out, σ = identity; init = glorot_uniform,\n",
       "     stride = 1, pad = 0, dilation = 1)\n",
       "```\n",
       "\n",
       "Standard convolutional layer. `size` should be a tuple like `(2, 2)`. `in` and `out` specify the number of input and output channels respectively.\n",
       "\n",
       "Data should be stored in WHCN order (width, height, # channels, batch size). In other words, a 100×100 RGB image would be a `100×100×3×1` array, and a batch of 50 would be a `100×100×3×50` array.\n",
       "\n",
       "# Examples\n",
       "\n",
       "Apply a `Conv` layer to a 1-channel input using a 2×2 window size, giving us a 16-channel output. Output is activated with ReLU.\n",
       "\n",
       "```julia\n",
       "size = (2,2)\n",
       "in = 1\n",
       "out = 16\n",
       "Conv(size, in => out, relu)\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  Conv(size, in => out, σ = identity; init = glorot_uniform,\u001b[39m\n",
       "\u001b[36m       stride = 1, pad = 0, dilation = 1)\u001b[39m\n",
       "\n",
       "  Standard convolutional layer. \u001b[36msize\u001b[39m should be a tuple like \u001b[36m(2, 2)\u001b[39m. \u001b[36min\u001b[39m and \u001b[36mout\u001b[39m\n",
       "  specify the number of input and output channels respectively.\n",
       "\n",
       "  Data should be stored in WHCN order (width, height, # channels, batch size).\n",
       "  In other words, a 100×100 RGB image would be a \u001b[36m100×100×3×1\u001b[39m array, and a\n",
       "  batch of 50 would be a \u001b[36m100×100×3×50\u001b[39m array.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  Apply a \u001b[36mConv\u001b[39m layer to a 1-channel input using a 2×2 window size, giving us a\n",
       "  16-channel output. Output is activated with ReLU.\n",
       "\n",
       "\u001b[36m  size = (2,2)\u001b[39m\n",
       "\u001b[36m  in = 1\u001b[39m\n",
       "\u001b[36m  out = 16\u001b[39m\n",
       "\u001b[36m  Conv(size, in => out, relu)\u001b[39m"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = randn(32, 32, 1, 2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 4, 2)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = l(x);\n",
    "size(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`outdims` is very useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Flux.outdims(l, size(x))"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.4.2",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
